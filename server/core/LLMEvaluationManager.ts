/**
 * LLM EVALUATION MANAGER
 * Comprehensive evaluation system with golden sets, accuracy tracking, and performance analytics
 */

export interface EvaluationSuite {
  id: string;
  name: string;
  description: string;
  category: EvaluationCategory;
  goldenSets: GoldenSet[];
  metrics: EvaluationMetric[];
  schedule: EvaluationSchedule;
  status: SuiteStatus;
  createdAt: Date;
  lastRunAt?: Date;
  nextRunAt?: Date;
}

export type EvaluationCategory = 
  | 'general' | 'coding' | 'math' | 'reasoning' | 'creative_writing'
  | 'summarization' | 'translation' | 'qa' | 'classification' | 'extraction'
  | 'safety' | 'bias' | 'toxicity' | 'hallucination';

export interface GoldenSet {
  id: string;
  name: string;
  description: string;
  category: EvaluationCategory;
  testCases: TestCase[];
  metadata: {
    version: string;
    source: string;
    license?: string;
    difficulty: 'easy' | 'medium' | 'hard' | 'expert';
    language: string;
    domain?: string;
  };
  createdAt: Date;
  updatedAt: Date;
}

export interface TestCase {
  id: string;
  input: {
    prompt: string;
    context?: string;
    instructions?: string;
    constraints?: Record<string, any>;
  };
  expectedOutput: {
    content: string;
    format?: 'text' | 'json' | 'code' | 'markdown';
    criteria: EvaluationCriteria[];
  };
  metadata: {
    difficulty: number; // 1-10
    tags: string[];
    source?: string;
    humanVerified: boolean;
  };
}

export interface EvaluationCriteria {
  type: CriteriaType;
  weight: number; // 0-1
  threshold?: number;
  description: string;
}

export type CriteriaType = 
  | 'semantic_similarity' | 'exact_match' | 'contains_keywords' | 'format_compliance'
  | 'factual_accuracy' | 'completeness' | 'relevance' | 'coherence'
  | 'safety' | 'bias_detection' | 'toxicity' | 'hallucination_detection';

export interface EvaluationMetric {
  name: string;
  type: MetricType;
  aggregation: AggregationType;
  weight: number;
  target?: number; // Target value for the metric
}

export type MetricType = 
  | 'accuracy' | 'precision' | 'recall' | 'f1_score' | 'bleu_score' | 'rouge_score'
  | 'semantic_similarity' | 'latency' | 'cost' | 'safety_score' | 'bias_score';

export type AggregationType = 'mean' | 'median' | 'max' | 'min' | 'weighted_mean';

export interface EvaluationSchedule {
  enabled: boolean;
  frequency: ScheduleFrequency;
  time?: string; // HH:mm format
  days?: number[]; // 0-6 for Sunday-Saturday
  timezone: string;
  triggers: ScheduleTrigger[];
}

export type ScheduleFrequency = 'daily' | 'weekly' | 'monthly' | 'on_demand' | 'continuous';

export interface ScheduleTrigger {
  type: TriggerType;
  condition: any;
  enabled: boolean;
}

export type TriggerType = 
  | 'model_update' | 'prompt_change' | 'performance_degradation' 
  | 'new_golden_set' | 'threshold_breach' | 'manual';

export type SuiteStatus = 'active' | 'paused' | 'archived' | 'running' | 'failed';

export interface EvaluationRun {
  id: string;
  suiteId: string;
  suiteName: string;
  modelId: string;
  modelName: string;
  status: RunStatus;
  progress: RunProgress;
  results: EvaluationResults;
  metadata: RunMetadata;
  startedAt: Date;
  completedAt?: Date;
  duration?: number; // milliseconds
}

export type RunStatus = 'pending' | 'running' | 'completed' | 'failed' | 'cancelled';

export interface RunProgress {
  totalTestCases: number;
  completedTestCases: number;
  successfulTestCases: number;
  failedTestCases: number;
  currentTestCase?: string;
  estimatedTimeRemaining?: number; // milliseconds
}

export interface EvaluationResults {
  overall: OverallResults;
  byCategory: Record<EvaluationCategory, CategoryResults>;
  byMetric: Record<string, MetricResults>;
  testCaseResults: TestCaseResult[];
  insights: EvaluationInsight[];
}

export interface OverallResults {
  score: number; // 0-100
  grade: 'A' | 'B' | 'C' | 'D' | 'F';
  passed: boolean;
  passRate: number; // 0-100
  totalCost: number;
  averageLatency: number;
}

export interface CategoryResults {
  score: number;
  testCases: number;
  passed: number;
  failed: number;
  improvements: string[];
  regressions: string[];
}

export interface MetricResults {
  value: number;
  target?: number;
  delta?: number; // Change from previous run
  trend: 'improving' | 'stable' | 'declining';
  distribution: number[]; // For visualization
}

export interface TestCaseResult {
  testCaseId: string;
  input: any;
  expectedOutput: any;
  actualOutput: any;
  passed: boolean;
  score: number; // 0-100
  criteriaResults: CriteriaResult[];
  latency: number;
  cost: number;
  error?: string;
}

export interface CriteriaResult {
  type: CriteriaType;
  passed: boolean;
  score: number;
  details: string;
}

export interface EvaluationInsight {
  type: InsightType;
  severity: 'info' | 'warning' | 'critical';
  title: string;
  description: string;
  recommendation?: string;
  affectedTestCases?: string[];
  confidence: number; // 0-100
}

export type InsightType = 
  | 'performance_regression' | 'accuracy_improvement' | 'bias_detected'
  | 'safety_concern' | 'cost_increase' | 'latency_issue' | 'format_error'
  | 'hallucination_detected' | 'consistency_issue';

export interface RunMetadata {
  triggeredBy: TriggerType;
  environment: string;
  modelVersion?: string;
  promptVersion?: string;
  temperature?: number;
  maxTokens?: number;
  configuration: Record<string, any>;
}

export interface ComparisonReport {
  id: string;
  name: string;
  baselineRun: string;
  comparisonRuns: string[];
  metrics: ComparisonMetric[];
  insights: ComparisonInsight[];
  createdAt: Date;
}

export interface ComparisonMetric {
  name: string;
  baseline: number;
  comparisons: Array<{
    runId: string;
    value: number;
    delta: number;
    deltaPercent: number;
  }>;
  winner?: string;
}

export interface ComparisonInsight {
  type: 'improvement' | 'regression' | 'neutral';
  metric: string;
  description: string;
  significance: 'low' | 'medium' | 'high';
}

export interface ABTestConfig {
  id: string;
  name: string;
  description: string;
  variants: ABTestVariant[];
  trafficAllocation: Record<string, number>; // variant -> percentage
  successMetrics: string[];
  duration: number; // days
  status: ABTestStatus;
  startedAt?: Date;
  endedAt?: Date;
}

export interface ABTestVariant {
  id: string;
  name: string;
  modelId: string;
  promptTemplate?: string;
  configuration: Record<string, any>;
}

export type ABTestStatus = 'draft' | 'running' | 'completed' | 'cancelled';

class LLMEvaluationManager {
  private suites = new Map<string, EvaluationSuite>();
  private runs = new Map<string, EvaluationRun>();
  private goldenSets = new Map<string, GoldenSet>();
  private comparisons = new Map<string, ComparisonReport>();
  private abTests = new Map<string, ABTestConfig>();
  
  private readonly maxRuns = 1000;
  private runningEvaluations = new Set<string>();

  constructor() {
    this.initializeDefaultSuites();
    this.initializeDefaultGoldenSets();
    this.startScheduler();
    console.log('üìä LLM Evaluation Manager initialized');
  }

  /**
   * Create a new evaluation suite
   */
  createEvaluationSuite(data: {
    name: string;
    description: string;
    category: EvaluationCategory;
    goldenSetIds: string[];
    metrics: EvaluationMetric[];
    schedule?: Partial<EvaluationSchedule>;
  }): EvaluationSuite {
    const id = this.generateSuiteId();
    
    const goldenSets = data.goldenSetIds
      .map(id => this.goldenSets.get(id))
      .filter(Boolean) as GoldenSet[];

    if (goldenSets.length === 0) {
      throw new Error('At least one valid golden set is required');
    }

    const suite: EvaluationSuite = {
      id,
      name: data.name,
      description: data.description,
      category: data.category,
      goldenSets,
      metrics: data.metrics,
      schedule: {
        enabled: false,
        frequency: 'on_demand',
        timezone: 'UTC',
        triggers: [],
        ...data.schedule
      },
      status: 'active',
      createdAt: new Date()
    };

    this.suites.set(id, suite);
    console.log(`üìä Created evaluation suite: ${data.name}`);
    
    return suite;
  }

  /**
   * Run an evaluation suite against a model
   */
  async runEvaluation(suiteId: string, modelId: string, options?: {
    promptVersion?: string;
    configuration?: Record<string, any>;
    subset?: number; // Run only N test cases
  }): Promise<EvaluationRun> {
    const suite = this.suites.get(suiteId);
    if (!suite) {
      throw new Error(`Evaluation suite not found: ${suiteId}`);
    }

    if (this.runningEvaluations.has(`${suiteId}:${modelId}`)) {
      throw new Error('Evaluation already running for this suite and model');
    }

    const runId = this.generateRunId();
    
    // Collect all test cases
    const allTestCases = suite.goldenSets.flatMap(gs => gs.testCases);
    const testCases = options?.subset 
      ? allTestCases.slice(0, options.subset)
      : allTestCases;

    const run: EvaluationRun = {
      id: runId,
      suiteId,
      suiteName: suite.name,
      modelId,
      modelName: await this.getModelName(modelId),
      status: 'running',
      progress: {
        totalTestCases: testCases.length,
        completedTestCases: 0,
        successfulTestCases: 0,
        failedTestCases: 0
      },
      results: this.initializeResults(),
      metadata: {
        triggeredBy: 'manual',
        environment: 'production',
        promptVersion: options?.promptVersion,
        configuration: options?.configuration || {}
      },
      startedAt: new Date()
    };

    this.runs.set(runId, run);
    this.runningEvaluations.add(`${suiteId}:${modelId}`);

    console.log(`üß™ Starting evaluation run ${runId} for suite ${suite.name}`);

    // Execute evaluation asynchronously
    this.executeEvaluation(run, testCases).catch(error => {
      console.error(`‚ùå Evaluation run ${runId} failed:`, error);
      run.status = 'failed';
      this.runningEvaluations.delete(`${suiteId}:${modelId}`);
    });

    return run;
  }

  /**
   * Create a golden set from examples
   */
  createGoldenSet(data: {
    name: string;
    description: string;
    category: EvaluationCategory;
    testCases: Array<{
      prompt: string;
      expectedOutput: string;
      criteria: EvaluationCriteria[];
      difficulty?: number;
      tags?: string[];
    }>;
    metadata?: Partial<GoldenSet['metadata']>;
  }): GoldenSet {
    const id = this.generateGoldenSetId();
    
    const testCases: TestCase[] = data.testCases.map((tc, index) => ({
      id: `${id}_case_${index}`,
      input: {
        prompt: tc.prompt,
        context: undefined,
        instructions: undefined
      },
      expectedOutput: {
        content: tc.expectedOutput,
        format: 'text',
        criteria: tc.criteria
      },
      metadata: {
        difficulty: tc.difficulty || 5,
        tags: tc.tags || [],
        humanVerified: true
      }
    }));

    const goldenSet: GoldenSet = {
      id,
      name: data.name,
      description: data.description,
      category: data.category,
      testCases,
      metadata: {
        version: '1.0',
        source: 'manual',
        difficulty: 'medium',
        language: 'en',
        ...data.metadata
      },
      createdAt: new Date(),
      updatedAt: new Date()
    };

    this.goldenSets.set(id, goldenSet);
    console.log(`üìù Created golden set: ${data.name} with ${testCases.length} test cases`);
    
    return goldenSet;
  }

  /**
   * Compare multiple evaluation runs
   */
  createComparison(data: {
    name: string;
    baselineRunId: string;
    comparisonRunIds: string[];
    metrics?: string[];
  }): ComparisonReport {
    const baselineRun = this.runs.get(data.baselineRunId);
    if (!baselineRun) {
      throw new Error(`Baseline run not found: ${data.baselineRunId}`);
    }

    const comparisonRuns = data.comparisonRunIds
      .map(id => this.runs.get(id))
      .filter(Boolean) as EvaluationRun[];

    if (comparisonRuns.length === 0) {
      throw new Error('At least one comparison run is required');
    }

    const metrics = data.metrics || Object.keys(baselineRun.results.byMetric);
    const comparisonMetrics: ComparisonMetric[] = metrics.map(metric => {
      const baseline = baselineRun.results.byMetric[metric]?.value || 0;
      
      const comparisons = comparisonRuns.map(run => {
        const value = run.results.byMetric[metric]?.value || 0;
        const delta = value - baseline;
        const deltaPercent = baseline !== 0 ? (delta / baseline) * 100 : 0;
        
        return {
          runId: run.id,
          value,
          delta,
          deltaPercent
        };
      });

      // Determine winner (highest value for most metrics)
      const winner = comparisons.reduce((best, current) => 
        current.value > best.value ? current : best
      );

      return {
        name: metric,
        baseline,
        comparisons,
        winner: winner.value > baseline ? winner.runId : data.baselineRunId
      };
    });

    // Generate insights
    const insights = this.generateComparisonInsights(comparisonMetrics);

    const comparison: ComparisonReport = {
      id: this.generateComparisonId(),
      name: data.name,
      baselineRun: data.baselineRunId,
      comparisonRuns: data.comparisonRunIds,
      metrics: comparisonMetrics,
      insights,
      createdAt: new Date()
    };

    this.comparisons.set(comparison.id, comparison);
    console.log(`üìà Created comparison report: ${data.name}`);
    
    return comparison;
  }

  /**
   * Start an A/B test
   */
  startABTest(config: Omit<ABTestConfig, 'id' | 'status' | 'startedAt'>): ABTestConfig {
    const id = this.generateABTestId();
    
    const abTest: ABTestConfig = {
      ...config,
      id,
      status: 'running',
      startedAt: new Date()
    };

    this.abTests.set(id, abTest);
    
    // Schedule automatic completion
    setTimeout(() => {
      this.completeABTest(id);
    }, config.duration * 24 * 60 * 60 * 1000);

    console.log(`üß™ Started A/B test: ${config.name}`);
    return abTest;
  }

  /**
   * Get evaluation analytics and dashboard data
   */
  getAnalytics(timeframe?: { start: Date; end: Date }): {
    overview: {
      totalRuns: number;
      successfulRuns: number;
      averageScore: number;
      trendsDirection: 'improving' | 'stable' | 'declining';
    };
    performance: {
      scoresByCategory: Record<EvaluationCategory, number>;
      scoresByModel: Record<string, number>;
      trendsOverTime: Array<{
        date: string;
        averageScore: number;
        totalRuns: number;
      }>;
    };
    insights: EvaluationInsight[];
    recommendations: string[];
    activeABTests: ABTestConfig[];
  } {
    let runs = Array.from(this.runs.values());
    
    if (timeframe) {
      runs = runs.filter(run => 
        run.startedAt >= timeframe.start && 
        run.startedAt <= timeframe.end
      );
    }

    const completedRuns = runs.filter(run => run.status === 'completed');
    const averageScore = completedRuns.length > 0 ?
      completedRuns.reduce((sum, run) => sum + run.results.overall.score, 0) / completedRuns.length : 0;

    // Calculate trends
    const recentRuns = completedRuns.slice(-10);
    const olderRuns = completedRuns.slice(-20, -10);
    const recentAverage = recentRuns.length > 0 ?
      recentRuns.reduce((sum, run) => sum + run.results.overall.score, 0) / recentRuns.length : 0;
    const olderAverage = olderRuns.length > 0 ?
      olderRuns.reduce((sum, run) => sum + run.results.overall.score, 0) / olderRuns.length : 0;
    
    const trendsDirection: 'improving' | 'stable' | 'declining' = 
      recentAverage > olderAverage + 2 ? 'improving' :
      recentAverage < olderAverage - 2 ? 'declining' : 'stable';

    // Aggregate by category
    const scoresByCategory: Record<EvaluationCategory, number> = {} as any;
    const categoryRuns: Record<EvaluationCategory, number> = {} as any;
    
    completedRuns.forEach(run => {
      Object.entries(run.results.byCategory).forEach(([category, results]) => {
        if (!scoresByCategory[category as EvaluationCategory]) {
          scoresByCategory[category as EvaluationCategory] = 0;
          categoryRuns[category as EvaluationCategory] = 0;
        }
        scoresByCategory[category as EvaluationCategory] += results.score;
        categoryRuns[category as EvaluationCategory]++;
      });
    });

    // Average by category
    Object.keys(scoresByCategory).forEach(category => {
      const cat = category as EvaluationCategory;
      scoresByCategory[cat] = scoresByCategory[cat] / categoryRuns[cat];
    });

    // Aggregate by model
    const scoresByModel: Record<string, number> = {};
    const modelRuns: Record<string, number> = {};
    
    completedRuns.forEach(run => {
      if (!scoresByModel[run.modelId]) {
        scoresByModel[run.modelId] = 0;
        modelRuns[run.modelId] = 0;
      }
      scoresByModel[run.modelId] += run.results.overall.score;
      modelRuns[run.modelId]++;
    });

    // Average by model
    Object.keys(scoresByModel).forEach(model => {
      scoresByModel[model] = scoresByModel[model] / modelRuns[model];
    });

    // Generate trends over time (simplified)
    const trendsOverTime = [
      { date: '2024-01-01', averageScore: 82.5, totalRuns: 15 },
      { date: '2024-01-02', averageScore: 84.2, totalRuns: 18 },
      { date: '2024-01-03', averageScore: 83.8, totalRuns: 22 },
      { date: '2024-01-04', averageScore: 85.1, totalRuns: 19 },
      { date: '2024-01-05', averageScore: 86.3, totalRuns: 25 }
    ];

    // Collect insights from recent runs
    const insights = completedRuns
      .flatMap(run => run.results.insights)
      .slice(0, 10);

    // Generate recommendations
    const recommendations = this.generateRecommendations(completedRuns);

    // Get active A/B tests
    const activeABTests = Array.from(this.abTests.values())
      .filter(test => test.status === 'running');

    return {
      overview: {
        totalRuns: runs.length,
        successfulRuns: completedRuns.length,
        averageScore,
        trendsDirection
      },
      performance: {
        scoresByCategory,
        scoresByModel,
        trendsOverTime
      },
      insights,
      recommendations,
      activeABTests
    };
  }

  /**
   * Get evaluation suite details
   */
  getEvaluationSuite(suiteId: string): EvaluationSuite | null {
    return this.suites.get(suiteId) || null;
  }

  /**
   * List all evaluation suites
   */
  listEvaluationSuites(): EvaluationSuite[] {
    return Array.from(this.suites.values());
  }

  /**
   * Get evaluation run details
   */
  getEvaluationRun(runId: string): EvaluationRun | null {
    return this.runs.get(runId) || null;
  }

  /**
   * List evaluation runs with filtering
   */
  listEvaluationRuns(filters?: {
    suiteId?: string;
    modelId?: string;
    status?: RunStatus;
    timeRange?: { start: Date; end: Date };
  }): EvaluationRun[] {
    let runs = Array.from(this.runs.values());

    if (filters) {
      if (filters.suiteId) {
        runs = runs.filter(run => run.suiteId === filters.suiteId);
      }
      if (filters.modelId) {
        runs = runs.filter(run => run.modelId === filters.modelId);
      }
      if (filters.status) {
        runs = runs.filter(run => run.status === filters.status);
      }
      if (filters.timeRange) {
        runs = runs.filter(run => 
          run.startedAt >= filters.timeRange!.start && 
          run.startedAt <= filters.timeRange!.end
        );
      }
    }

    return runs.sort((a, b) => b.startedAt.getTime() - a.startedAt.getTime());
  }

  /**
   * Get golden set details
   */
  getGoldenSet(setId: string): GoldenSet | null {
    return this.goldenSets.get(setId) || null;
  }

  /**
   * List all golden sets
   */
  listGoldenSets(): GoldenSet[] {
    return Array.from(this.goldenSets.values());
  }

  // Private helper methods

  private async executeEvaluation(run: EvaluationRun, testCases: TestCase[]): Promise<void> {
    try {
      const results: TestCaseResult[] = [];
      
      for (let i = 0; i < testCases.length; i++) {
        const testCase = testCases[i];
        
        // Update progress
        run.progress.currentTestCase = testCase.id;
        
        try {
          // Execute test case (simulate LLM call)
          const result = await this.executeTestCase(testCase, run.modelId);
          results.push(result);
          
          if (result.passed) {
            run.progress.successfulTestCases++;
          } else {
            run.progress.failedTestCases++;
          }
          
        } catch (error) {
          run.progress.failedTestCases++;
          results.push({
            testCaseId: testCase.id,
            input: testCase.input,
            expectedOutput: testCase.expectedOutput,
            actualOutput: null,
            passed: false,
            score: 0,
            criteriaResults: [],
            latency: 0,
            cost: 0,
            error: error.message
          });
        }
        
        run.progress.completedTestCases = i + 1;
        
        // Small delay to simulate processing
        await this.delay(100);
      }

      // Calculate final results
      run.results = this.calculateResults(results, run);
      run.status = 'completed';
      run.completedAt = new Date();
      run.duration = run.completedAt.getTime() - run.startedAt.getTime();

      console.log(`‚úÖ Evaluation run ${run.id} completed: ${run.results.overall.score.toFixed(1)}% score`);

    } catch (error) {
      run.status = 'failed';
      console.error(`‚ùå Evaluation run ${run.id} failed:`, error);
    } finally {
      this.runningEvaluations.delete(`${run.suiteId}:${run.modelId}`);
    }
  }

  private async executeTestCase(testCase: TestCase, modelId: string): Promise<TestCaseResult> {
    const startTime = Date.now();
    
    // Simulate LLM API call
    const actualOutput = await this.simulateLLMCall(testCase.input.prompt, modelId);
    
    const latency = Date.now() - startTime;
    const cost = this.estimateCost(testCase.input.prompt, actualOutput, modelId);
    
    // Evaluate against criteria
    const criteriaResults = testCase.expectedOutput.criteria.map(criteria => 
      this.evaluateCriteria(criteria, testCase.expectedOutput.content, actualOutput)
    );
    
    const overallScore = criteriaResults.reduce((sum, cr) => 
      sum + (cr.score * testCase.expectedOutput.criteria.find(c => c.type === cr.type)!.weight), 0
    );
    
    const passed = criteriaResults.every(cr => cr.passed);
    
    return {
      testCaseId: testCase.id,
      input: testCase.input,
      expectedOutput: testCase.expectedOutput,
      actualOutput,
      passed,
      score: overallScore * 100,
      criteriaResults,
      latency,
      cost
    };
  }

  private async simulateLLMCall(prompt: string, modelId: string): Promise<string> {
    // Simulate different response quality based on model
    const modelQuality: Record<string, number> = {
      'openai-gpt-4': 0.95,
      'openai-gpt-4o-mini': 0.85,
      'anthropic-claude-3-haiku': 0.80,
      'google-gemini-pro': 0.88
    };
    
    const quality = modelQuality[modelId] || 0.75;
    const randomFactor = 0.8 + Math.random() * 0.4; // 0.8 to 1.2
    
    // Simulate response generation
    if (prompt.includes('classify')) {
      return Math.random() < quality * randomFactor ? 'Category: Positive' : 'Category: Neutral';
    } else if (prompt.includes('summarize')) {
      return quality * randomFactor > 0.8 ? 
        'This is a comprehensive and accurate summary of the content.' :
        'This is a summary.';
    } else if (prompt.includes('translate')) {
      return quality * randomFactor > 0.9 ?
        'Bonjour, comment allez-vous?' :
        'Bonjour, comment vous?';
    } else {
      return quality * randomFactor > 0.85 ?
        'This is a high-quality, accurate response that meets all criteria.' :
        'This is a response.';
    }
  }

  private evaluateCriteria(
    criteria: EvaluationCriteria,
    expected: string,
    actual: string
  ): CriteriaResult {
    let score = 0;
    let details = '';
    
    switch (criteria.type) {
      case 'semantic_similarity':
        // Simple similarity check (would use embeddings in production)
        const similarity = this.calculateSimilarity(expected, actual);
        score = similarity;
        details = `Semantic similarity: ${(similarity * 100).toFixed(1)}%`;
        break;
        
      case 'exact_match':
        score = expected.toLowerCase().trim() === actual.toLowerCase().trim() ? 1 : 0;
        details = score === 1 ? 'Exact match found' : 'No exact match';
        break;
        
      case 'contains_keywords':
        const keywords = expected.toLowerCase().split(' ');
        const actualLower = actual.toLowerCase();
        const matchedKeywords = keywords.filter(keyword => actualLower.includes(keyword));
        score = matchedKeywords.length / keywords.length;
        details = `Matched ${matchedKeywords.length}/${keywords.length} keywords`;
        break;
        
      case 'format_compliance':
        // Check if output matches expected format
        score = this.checkFormatCompliance(expected, actual);
        details = score > 0.8 ? 'Format compliant' : 'Format issues detected';
        break;
        
      default:
        score = 0.8; // Default score for unimplemented criteria
        details = 'Basic evaluation performed';
    }
    
    const passed = score >= (criteria.threshold || 0.7);
    
    return {
      type: criteria.type,
      passed,
      score,
      details
    };
  }

  private calculateSimilarity(text1: string, text2: string): number {
    // Simple word overlap similarity (would use embeddings in production)
    const words1 = new Set(text1.toLowerCase().split(/\s+/));
    const words2 = new Set(text2.toLowerCase().split(/\s+/));
    
    const intersection = new Set([...words1].filter(x => words2.has(x)));
    const union = new Set([...words1, ...words2]);
    
    return intersection.size / union.size;
  }

  private checkFormatCompliance(expected: string, actual: string): number {
    // Check basic format features (JSON, structure, etc.)
    if (expected.includes('{') && expected.includes('}')) {
      // Expected JSON format
      try {
        JSON.parse(actual);
        return 1.0; // Valid JSON
      } catch {
        return 0.2; // Invalid JSON
      }
    }
    
    if (expected.includes('Category:')) {
      // Expected classification format
      return actual.includes('Category:') ? 1.0 : 0.5;
    }
    
    return 0.8; // Default compliance score
  }

  private estimateCost(prompt: string, output: string, modelId: string): number {
    const promptTokens = Math.ceil(prompt.length / 4);
    const outputTokens = Math.ceil(output.length / 4);
    
    // Model pricing (simplified)
    const pricing: Record<string, { input: number; output: number }> = {
      'openai-gpt-4': { input: 0.03, output: 0.06 },
      'openai-gpt-4o-mini': { input: 0.00015, output: 0.0006 },
      'anthropic-claude-3-haiku': { input: 0.00025, output: 0.00125 },
      'google-gemini-pro': { input: 0.001, output: 0.002 }
    };
    
    const modelPricing = pricing[modelId] || { input: 0.001, output: 0.002 };
    
    return (promptTokens / 1000) * modelPricing.input + (outputTokens / 1000) * modelPricing.output;
  }

  private calculateResults(testCaseResults: TestCaseResult[], run: EvaluationRun): EvaluationResults {
    const suite = this.suites.get(run.suiteId)!;
    
    // Overall results
    const overallScore = testCaseResults.reduce((sum, r) => sum + r.score, 0) / testCaseResults.length;
    const passRate = (testCaseResults.filter(r => r.passed).length / testCaseResults.length) * 100;
    const totalCost = testCaseResults.reduce((sum, r) => sum + r.cost, 0);
    const averageLatency = testCaseResults.reduce((sum, r) => sum + r.latency, 0) / testCaseResults.length;
    
    const overall: OverallResults = {
      score: overallScore,
      grade: this.getGrade(overallScore),
      passed: passRate >= 70, // 70% pass rate threshold
      passRate,
      totalCost,
      averageLatency
    };

    // Results by category
    const byCategory: Record<EvaluationCategory, CategoryResults> = {} as any;
    
    suite.goldenSets.forEach(gs => {
      const categoryResults = testCaseResults.filter(r => 
        gs.testCases.some(tc => tc.id === r.testCaseId)
      );
      
      if (categoryResults.length > 0) {
        const score = categoryResults.reduce((sum, r) => sum + r.score, 0) / categoryResults.length;
        const passed = categoryResults.filter(r => r.passed).length;
        
        byCategory[gs.category] = {
          score,
          testCases: categoryResults.length,
          passed,
          failed: categoryResults.length - passed,
          improvements: [],
          regressions: []
        };
      }
    });

    // Results by metric
    const byMetric: Record<string, MetricResults> = {};
    
    suite.metrics.forEach(metric => {
      let value = 0;
      
      switch (metric.type) {
        case 'accuracy':
          value = overall.score;
          break;
        case 'latency':
          value = averageLatency;
          break;
        case 'cost':
          value = totalCost;
          break;
        default:
          value = overall.score; // Default to overall score
      }
      
      byMetric[metric.name] = {
        value,
        target: metric.target,
        trend: 'stable', // Would calculate from history
        distribution: this.generateDistribution(value)
      };
    });

    // Generate insights
    const insights = this.generateInsights(testCaseResults, overall);

    return {
      overall,
      byCategory,
      byMetric,
      testCaseResults,
      insights
    };
  }

  private getGrade(score: number): 'A' | 'B' | 'C' | 'D' | 'F' {
    if (score >= 90) return 'A';
    if (score >= 80) return 'B';
    if (score >= 70) return 'C';
    if (score >= 60) return 'D';
    return 'F';
  }

  private generateInsights(results: TestCaseResult[], overall: OverallResults): EvaluationInsight[] {
    const insights: EvaluationInsight[] = [];
    
    // Check for performance issues
    if (overall.averageLatency > 5000) {
      insights.push({
        type: 'latency_issue',
        severity: 'warning',
        title: 'High Latency Detected',
        description: `Average response time of ${overall.averageLatency.toFixed(0)}ms exceeds threshold`,
        recommendation: 'Consider using a faster model for latency-sensitive applications',
        confidence: 90
      });
    }
    
    // Check for cost issues
    if (overall.totalCost > 1.0) {
      insights.push({
        type: 'cost_increase',
        severity: 'warning',
        title: 'High Evaluation Cost',
        description: `Total cost of $${overall.totalCost.toFixed(2)} is above budget`,
        recommendation: 'Consider using a more cost-effective model for routine evaluations',
        confidence: 85
      });
    }
    
    // Check for accuracy issues
    const lowScoreResults = results.filter(r => r.score < 60);
    if (lowScoreResults.length > results.length * 0.2) {
      insights.push({
        type: 'performance_regression',
        severity: 'critical',
        title: 'Accuracy Concerns',
        description: `${lowScoreResults.length} test cases scored below 60%`,
        recommendation: 'Review and improve prompts or consider model fine-tuning',
        affectedTestCases: lowScoreResults.map(r => r.testCaseId),
        confidence: 95
      });
    }
    
    return insights;
  }

  private generateDistribution(value: number): number[] {
    // Generate a simple distribution around the value for visualization
    const distribution = [];
    for (let i = 0; i < 10; i++) {
      const variance = (Math.random() - 0.5) * value * 0.2;
      distribution.push(Math.max(0, value + variance));
    }
    return distribution;
  }

  private generateComparisonInsights(metrics: ComparisonMetric[]): ComparisonInsight[] {
    const insights: ComparisonInsight[] = [];
    
    metrics.forEach(metric => {
      const bestComparison = metric.comparisons.reduce((best, current) => 
        current.deltaPercent > best.deltaPercent ? current : best
      );
      
      if (Math.abs(bestComparison.deltaPercent) > 5) {
        insights.push({
          type: bestComparison.deltaPercent > 0 ? 'improvement' : 'regression',
          metric: metric.name,
          description: `${Math.abs(bestComparison.deltaPercent).toFixed(1)}% change in ${metric.name}`,
          significance: Math.abs(bestComparison.deltaPercent) > 20 ? 'high' : 
                      Math.abs(bestComparison.deltaPercent) > 10 ? 'medium' : 'low'
        });
      }
    });
    
    return insights;
  }

  private generateRecommendations(runs: EvaluationRun[]): string[] {
    const recommendations: string[] = [];
    
    if (runs.length === 0) {
      recommendations.push('Start by running evaluations on your most important use cases');
      return recommendations;
    }
    
    const averageScore = runs.reduce((sum, run) => sum + run.results.overall.score, 0) / runs.length;
    
    if (averageScore < 70) {
      recommendations.push('Consider improving prompt engineering or model selection');
    }
    
    if (averageScore > 90) {
      recommendations.push('Excellent performance! Consider expanding to more challenging test cases');
    }
    
    const recentRuns = runs.slice(0, 5);
    const avgLatency = recentRuns.reduce((sum, run) => sum + run.results.overall.averageLatency, 0) / recentRuns.length;
    
    if (avgLatency > 3000) {
      recommendations.push('Optimize for speed by using faster models or shorter prompts');
    }
    
    return recommendations;
  }

  private completeABTest(testId: string): void {
    const test = this.abTests.get(testId);
    if (test && test.status === 'running') {
      test.status = 'completed';
      test.endedAt = new Date();
      console.log(`üèÅ A/B test completed: ${test.name}`);
    }
  }

  private initializeDefaultSuites(): void {
    // Create a basic general evaluation suite
    const generalMetrics: EvaluationMetric[] = [
      { name: 'Overall Accuracy', type: 'accuracy', aggregation: 'mean', weight: 0.4, target: 85 },
      { name: 'Response Quality', type: 'semantic_similarity', aggregation: 'mean', weight: 0.3, target: 80 },
      { name: 'Cost Efficiency', type: 'cost', aggregation: 'mean', weight: 0.2 },
      { name: 'Speed', type: 'latency', aggregation: 'mean', weight: 0.1, target: 2000 }
    ];

    console.log('üìä Initialized default evaluation suites');
  }

  private initializeDefaultGoldenSets(): void {
    // Create sample golden sets
    const classificationSet = this.createGoldenSet({
      name: 'Sentiment Classification',
      description: 'Basic sentiment analysis test cases',
      category: 'classification',
      testCases: [
        {
          prompt: 'Classify the sentiment: I love this product!',
          expectedOutput: 'Category: Positive',
          criteria: [
            { type: 'contains_keywords', weight: 1.0, threshold: 0.8, description: 'Contains correct classification' }
          ],
          difficulty: 3,
          tags: ['sentiment', 'basic']
        },
        {
          prompt: 'Classify the sentiment: This is terrible.',
          expectedOutput: 'Category: Negative',
          criteria: [
            { type: 'contains_keywords', weight: 1.0, threshold: 0.8, description: 'Contains correct classification' }
          ],
          difficulty: 3,
          tags: ['sentiment', 'basic']
        }
      ]
    });

    console.log('üìù Initialized default golden sets');
  }

  private initializeResults(): EvaluationResults {
    return {
      overall: {
        score: 0,
        grade: 'F',
        passed: false,
        passRate: 0,
        totalCost: 0,
        averageLatency: 0
      },
      byCategory: {},
      byMetric: {},
      testCaseResults: [],
      insights: []
    };
  }

  private generateSuiteId(): string {
    return `suite_${Date.now()}_${Math.random().toString(36).slice(2)}`;
  }

  private generateRunId(): string {
    return `run_${Date.now()}_${Math.random().toString(36).slice(2)}`;
  }

  private generateGoldenSetId(): string {
    return `golden_${Date.now()}_${Math.random().toString(36).slice(2)}`;
  }

  private generateComparisonId(): string {
    return `comp_${Date.now()}_${Math.random().toString(36).slice(2)}`;
  }

  private generateABTestId(): string {
    return `ab_${Date.now()}_${Math.random().toString(36).slice(2)}`;
  }

  private async getModelName(modelId: string): Promise<string> {
    // In production, this would fetch from the model registry
    const modelNames: Record<string, string> = {
      'openai-gpt-4': 'GPT-4',
      'openai-gpt-4o-mini': 'GPT-4o Mini',
      'anthropic-claude-3-haiku': 'Claude 3 Haiku',
      'google-gemini-pro': 'Gemini Pro'
    };
    
    return modelNames[modelId] || modelId;
  }

  private async delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private startScheduler(): void {
    // Check for scheduled evaluations every hour
    setInterval(() => {
      const now = new Date();
      
      Array.from(this.suites.values()).forEach(suite => {
        if (suite.schedule.enabled && this.shouldRunScheduledEvaluation(suite, now)) {
          console.log(`‚è∞ Starting scheduled evaluation for suite: ${suite.name}`);
          // Would trigger evaluation here
        }
      });
    }, 60 * 60 * 1000); // Every hour
  }

  private shouldRunScheduledEvaluation(suite: EvaluationSuite, now: Date): boolean {
    if (!suite.schedule.enabled) return false;
    
    // Simple scheduling logic (would be more sophisticated in production)
    switch (suite.schedule.frequency) {
      case 'daily':
        return !suite.lastRunAt || 
               (now.getTime() - suite.lastRunAt.getTime()) > 24 * 60 * 60 * 1000;
      case 'weekly':
        return !suite.lastRunAt || 
               (now.getTime() - suite.lastRunAt.getTime()) > 7 * 24 * 60 * 60 * 1000;
      default:
        return false;
    }
  }
}

export const llmEvaluationManager = new LLMEvaluationManager();