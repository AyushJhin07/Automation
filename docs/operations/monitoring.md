# Monitoring Runbook

This runbook explains how to monitor the Automation API and queue layer using the production health endpoints. The probes are safe to call in any environment and are exposed under the `/api/production` namespace.

## Production Health Endpoints

### `GET /api/production/health`

Use this endpoint for a full snapshot that surfaces database, LLM, workflow, memory, queue, and dependency health. The handler aggregates subsystem checks, downgrading the overall status to `degraded` when any probe returns `warn` and to `unhealthy` when a probe fails; the HTTP status code flips to `503` in the unhealthy case.ã€F:server/routes/production-health.tsâ€ L60-L103ã€‘

### `GET /api/production/ready`

Use this probe for fast readiness checks (Kubernetes-style) and in deployment automation.

* Successful responses return HTTP 200 with `ready: true` once all quick checks pass (LLM configuration, production environment flag, dependency stub, and queue durability). The response body also includes `checks.queueDetails` so operators can inspect the underlying queue probe without issuing another request.ã€F:server/routes/production-health.tsâ€ L144-L165ã€‘
* When Redis is unavailable, the queue check fails with `status: "fail"`, `durable: true`, and a message like `Redis ping failed: ECONNREFUSED`, producing an overall `ready: false` and HTTP 503. The probe logs remediation tips the first time connectivity fails.ã€F:server/services/QueueHealthService.tsâ€ L29-L71ã€‘
* When the queue is running in in-memory mode (no persistence), the readiness probe still returns HTTP 503 and reports `queue: false` with the message `Queue driver is running in non-durable in-memory mode. Jobs will not be persisted.`ã€F:server/services/QueueHealthService.tsâ€ L81-L98ã€‘

During local development `npm run dev:stack` now polls this endpoint and terminates the stack immediately when the queue check reports `false`, preventing workflows from running without a durable queue.ã€F:scripts/dev-stack.tsâ€ L1-L210ã€‘

### `GET /api/production/queue/heartbeat`

Use this endpoint to observe the execution worker heartbeat and queue depth in detail.

* When the dedicated worker is offline (no heartbeat observed), the probe returns HTTP 503 with `status.status: "fail"` and the message `Execution worker has not been started. Queue processing is offline.`ã€F:server/routes/production-health.tsâ€ L228-L267ã€‘
* When Redis is reachable but reports a queue failure (for example, authentication issues), the heartbeat endpoint surfaces the queue health failure message and also returns HTTP 503.ã€F:server/routes/production-health.tsâ€ L236-L242ã€‘
* If heartbeats are stale or a backlog is building, the endpoint downgrades to `warn` and still returns HTTP 503 so alerting can trigger while providing the stale lease count or queue depth in the payload.ã€F:server/routes/production-health.tsâ€ L244-L258ã€‘
* Healthy responses include queue depths, worker metadata, and the latest heartbeat age and return HTTP 200.ã€F:server/routes/production-health.tsâ€ L260-L285ã€‘

## Incident Verification Procedures

### Quick CLI Checks

* Verify the API is ready and Redis is durable:
  ```bash
  curl -fsS "${API_ORIGIN:-http://localhost:5000}/api/production/ready" \
    | jq '{ready: .ready, queueReady: .checks.queue, queueMessage: .checks.queueDetails.message}'
  ```
* Inspect the worker heartbeat and backlog:
  ```bash
  curl -fsS "${API_ORIGIN:-http://localhost:5000}/api/production/queue/heartbeat" \
    | jq '{status: .status.status, message: .status.message, latestHeartbeatAt: .worker.latestHeartbeatAt, queueDepths: .queueDepths}'
  ```
* Capture a one-liner suitable for CI/CD guards (exits non-zero on queue failure):
  ```bash
  curl -fsSL "${API_ORIGIN:-http://localhost:5000}/api/production/ready" \
    | jq -e 'select(.checks.queue == true) | .ready == true' >/dev/null
  ```

### Example Dashboard Panel

For Grafana or any HTTP-capable dashboard, configure a JSON data source panel that queries `/api/production/queue/heartbeat` on a short interval (15â€“30 seconds) and visualises `status.status` alongside the numeric queue depth. Combine a stat panel for the heartbeat status with a time-series panel charting `sum(map_values(.queueDepths[]; .waiting + .delayed))` so on-call engineers can immediately see when a backlog forms and whether workers are keeping up.

## Observability health checks & dashboards

### Health checks

- **SDK bootstrap:** Run `npm run observability:check` with production environment variables to verify that the OpenTelemetry SDK initialises against your collector before deployments. A non-zero exit code blocks rollouts when exporters are misconfigured or the collector is unavailable.
- **Trace/metric ingestion:** Monitor the log message `ðŸ“ˆ OpenTelemetry instrumentation initialized` emitted by the API and worker pods on startup. Absence of the message indicates instrumentation never completed.
- **Runtime exporters:** When Prometheus scraping is enabled, hit `http://<pod>:9464/metrics` and ensure the response includes `workflow_queue_depth` and `workflow_node_latency_ms` samples for the current tenant.

### Dashboards

- **Workflow latency:** Plot the histogram metric `workflow_node_latency_ms` by `workflow_id` and `node_id` to surface nodes that regress. Combine with a percentile transformation (p95/p99) to trigger alerts on slowdowns.
- **Queue depth and saturation:** Visualise the `workflow_queue_depth` observable gauge alongside `/api/production/queue/heartbeat` responses. Alert when waiting or delayed counts remain above baseline for longer than the SLA.
- **Error rates:** Use `http_request_duration_ms_count` divided by error-status counts to derive API error rate, and track `workflow.execute` queue failure counts via the `queue.job.duration_ms` histogram's failure attribute bucket (available in Jaeger trace summaries). Pair these with the production health endpoints so operators can cross-reference incidents quickly.
